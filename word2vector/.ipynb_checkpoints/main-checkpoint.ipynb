{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import word2vec.data_preprocess as data_preprocess\n",
    "import word2vec.modules as modules\n",
    "import word2vec.utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    \"\"\"This is config class that store all parameter about training model\"\"\"\n",
    "    def __init__(self, vocab_size, bag_size):\n",
    "        self.mode = True # True is skip gram mode\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = 1024\n",
    "        self.batch_size = 512\n",
    "        self.bag_size = bag_size\n",
    "        \n",
    "        self.lr = 0.00001\n",
    "        self.epochs = 30\n",
    "        \n",
    "        self.save_path = \"./ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model: modules.Word2VecModel, training_loader: data.DataLoader, device: torch.device, config: Config) -> float:\n",
    "    \"\"\"Train model with steps \n",
    "    \n",
    "    Args:\n",
    "        model: the model will be trained\n",
    "        training_loader: the dataloader of training dataset\n",
    "        device: the hardware of training\n",
    "        config: the training config\n",
    "    \n",
    "    Returns:\n",
    "        return avg loss in steps\n",
    "    \"\"\"\n",
    "    model.train() # enter train mode to refresh gradient\n",
    "    model.zero_grad()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n",
    "    loss_function = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    total_loss = 0\n",
    "    avg_loss = 0\n",
    "    \n",
    "    for step, (inputs, targets) in enumerate(training_loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        inputs = inputs.to(device)\n",
    "        targets = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        avg_loss = total_loss / (step + 1)\n",
    "        print(\"\\r\", \"Train step[{}/{}] loss:{}]\".format(step + 1, len(training_loader), avg_loss), end=\"\")\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(training_dataset, config):\n",
    "    \"\"\"The training process\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    training_loader = data.DataLoader(training_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "    \n",
    "    model = modules.Word2VecModel(config.vocab_size, config.embedding_dim, config.batch_size, config.bag_size, config.mode)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    for epoch in range(config.epochs):\n",
    "        print(\"Epoch [{}/{}]\".format(epoch + 1, config.epochs))\n",
    "        epoch_avg_loss = train_step(model, training_loader, device, config)\n",
    "        save_model(model, config.save_path, epoch_avg_loss, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    \"\"\"The test process\"\"\"\n",
    "    window_size = 2\n",
    "    training_set = read_data(\"./datasets/trainset.csv\")\n",
    "    \n",
    "    training_dataset = Word2VecDataset(training_set, window_size, True)\n",
    "    config = Config(len(training_dataset.word2idx), training_dataset.bag_size)\n",
    "    model = Word2VecModel(config.vocab_size, config.embedding_dim, config.batch_size, config.bag_size, config.mode)\n",
    "    model = load_model(model, \"./module/model_29_0.095\")\n",
    "\n",
    "    #print(training_dataset.word2idx)\n",
    "    word_idx = 14\n",
    "    indices = find_nearest(word_idx, 8, model)\n",
    "    nearest_words = [training_dataset.idx2word[i.item()] for i in indices]\n",
    "    print(training_dataset.idx2word[word_idx])\n",
    "    print(nearest_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"The main function\"\"\"\n",
    "    window_size = 2\n",
    "    training_set = utils.read_data(\"./datasets/trainset.csv\")\n",
    "    \n",
    "    training_dataset = data_preprocess.Word2VecDataset(training_set, window_size, True)\n",
    "    config = Config(len(training_dataset.word2idx), training_dataset.bag_size)\n",
    "    train(training_dataset, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "Makeing dictionary...\n",
      "Generating pair word data...\n",
      "Epoch [1/30]\n",
      " Train step[1/2560] loss:10.560616493225098]\n",
      " Train step[2/2560] loss:10.541088581085205]\n",
      " Train step[3/2560] loss:10.55698299407959]\n",
      " Train step[4/2560] loss:10.55586314201355]\n",
      " Train step[5/2560] loss:10.566840553283692]\n",
      " Train step[6/2560] loss:10.571931680043539]\n",
      " Train step[7/2560] loss:10.564355850219727]\n",
      " Train step[8/2560] loss:10.563223600387573]\n",
      " Train step[9/2560] loss:10.558608796861437]\n",
      " Train step[10/2560] loss:10.555917835235595]\n",
      " Train step[11/2560] loss:10.551509250294078]\n",
      " Train step[12/2560] loss:10.548487663269043]\n",
      " Train step[13/2560] loss:10.54336635883038]\n",
      " Train step[14/2560] loss:10.537161758967809]\n",
      " Train step[15/2560] loss:10.534756024678549]\n",
      " Train step[16/2560] loss:10.531975448131561]\n",
      " Train step[17/2560] loss:10.53107110191794]\n",
      " Train step[18/2560] loss:10.527318053775364]\n",
      " Train step[19/2560] loss:10.527482032775879]\n",
      " Train step[20/2560] loss:10.52500605583191]\n",
      " Train step[21/2560] loss:10.520235697428385]\n",
      " Train step[22/2560] loss:10.516947529532693]\n",
      " Train step[23/2560] loss:10.51389972023342]\n",
      " Train step[24/2560] loss:10.510304808616638]\n",
      " Train step[25/2560] loss:10.506858100891113]\n",
      " Train step[26/2560] loss:10.503820199232836]\n",
      " Train step[27/2560] loss:10.501059956020779]\n",
      " Train step[28/2560] loss:10.498106002807617]\n",
      " Train step[29/2560] loss:10.493300701009817]\n",
      " Train step[30/2560] loss:10.49024133682251]\n",
      " Train step[31/2560] loss:10.486899314388152]\n",
      " Train step[32/2560] loss:10.484692007303238]\n",
      " Train step[33/2560] loss:10.482264952226119]\n",
      " Train step[34/2560] loss:10.481112957000732]\n",
      " Train step[35/2560] loss:10.478231947762625]\n",
      " Train step[36/2560] loss:10.474726491504246]\n",
      " Train step[37/2560] loss:10.470387329926362]\n",
      " Train step[38/2560] loss:10.467548546038175]\n",
      " Train step[39/2560] loss:10.464151235727163]\n",
      " Train step[40/2560] loss:10.460805201530457]\n",
      " Train step[41/2560] loss:10.458517539791945]\n",
      " Train step[42/2560] loss:10.455545856839134]\n",
      " Train step[43/2560] loss:10.453367321990257]\n",
      " Train step[44/2560] loss:10.4515700340271]\n",
      " Train step[45/2560] loss:10.449291377597385]\n",
      " Train step[46/2560] loss:10.445476283197818]\n",
      " Train step[47/2560] loss:10.441593657148646]\n",
      " Train step[48/2560] loss:10.439950148264566]\n",
      " Train step[49/2560] loss:10.43735681261335]\n",
      " Train step[50/2560] loss:10.434926624298095]\n",
      " Train step[51/2560] loss:10.43229138617422]\n",
      " Train step[52/2560] loss:10.42991801408621]\n",
      " Train step[53/2560] loss:10.426439627161566]\n",
      " Train step[54/2560] loss:10.42405038409763]\n",
      " Train step[55/2560] loss:10.420439997586337]\n",
      " Train step[56/2560] loss:10.41712817123958]\n",
      " Train step[57/2560] loss:10.41418488820394]\n",
      " Train step[58/2560] loss:10.410784310307996]\n",
      " Train step[59/2560] loss:10.408315496929621]\n",
      " Train step[60/2560] loss:10.40467028617859]\n",
      " Train step[61/2560] loss:10.40112976949723]\n",
      " Train step[62/2560] loss:10.398130401488274]\n",
      " Train step[63/2560] loss:10.395467758178711]\n",
      " Train step[64/2560] loss:10.392558559775352]\n",
      " Train step[65/2560] loss:10.389679468595064]\n",
      " Train step[66/2560] loss:10.387181527686842]\n",
      " Train step[67/2560] loss:10.38381267661479]\n",
      " Train step[68/2560] loss:10.380606735453886]\n",
      " Train step[69/2560] loss:10.378677796626437]\n",
      " Train step[70/2560] loss:10.375193527766637]\n",
      " Train step[71/2560] loss:10.37224625869536]\n",
      " Train step[72/2560] loss:10.369281146261427]\n",
      " Train step[73/2560] loss:10.366351885338352]\n",
      " Train step[74/2560] loss:10.36327209988156]\n",
      " Train step[75/2560] loss:10.360274785359701]\n",
      " Train step[76/2560] loss:10.356722630952534]\n",
      " Train step[77/2560] loss:10.353501468509823]\n",
      " Train step[78/2560] loss:10.35047100751828]\n",
      " Train step[79/2560] loss:10.34753211540512]\n",
      " Train step[80/2560] loss:10.344422900676728]\n",
      " Train step[81/2560] loss:10.34136105761116]\n",
      " Train step[82/2560] loss:10.337317699339332]\n",
      " Train step[83/2560] loss:10.334543193679258]\n",
      " Train step[84/2560] loss:10.331741333007812]\n",
      " Train step[85/2560] loss:10.328666900185977]\n",
      " Train step[86/2560] loss:10.325904557871263]\n",
      " Train step[87/2560] loss:10.32322755353204]\n",
      " Train step[88/2560] loss:10.320329644463278]\n",
      " Train step[89/2560] loss:10.316809975699092]\n",
      " Train step[90/2560] loss:10.314411640167236]\n",
      " Train step[91/2560] loss:10.311339703234998]\n",
      " Train step[92/2560] loss:10.307986518611079]\n",
      " Train step[93/2560] loss:10.304568188164824]\n",
      " Train step[94/2560] loss:10.301601825876439]\n",
      " Train step[95/2560] loss:10.298472344247918]\n",
      " Train step[96/2560] loss:10.295673767725626]\n",
      " Train step[97/2560] loss:10.293020631849151]\n",
      " Train step[98/2560] loss:10.2897367477417]\n",
      " Train step[99/2560] loss:10.286957461424548]\n",
      " Train step[100/2560] loss:10.283917961120606]\n",
      " Train step[101/2560] loss:10.28074822567477]\n",
      " Train step[102/2560] loss:10.278016445683498]\n",
      " Train step[103/2560] loss:10.275322321549202]\n",
      " Train step[104/2560] loss:10.272385331300589]\n",
      " Train step[105/2560] loss:10.269574410574776]\n",
      " Train step[106/2560] loss:10.266471044072565]\n",
      " Train step[107/2560] loss:10.263275672342175]\n",
      " Train step[108/2560] loss:10.260261429680718]\n",
      " Train step[109/2560] loss:10.257329582074366]\n",
      " Train step[110/2560] loss:10.254399473016912]\n",
      " Train step[111/2560] loss:10.251579834534242]\n",
      " Train step[112/2560] loss:10.247964339596885]\n",
      " Train step[113/2560] loss:10.244630914873781]\n",
      " Train step[114/2560] loss:10.242025576139751]\n",
      " Train step[115/2560] loss:10.238606909047002]\n",
      " Train step[116/2560] loss:10.235765588694605]\n",
      " Train step[117/2560] loss:10.232346510275816]\n",
      " Train step[118/2560] loss:10.22978825488333]\n",
      " Train step[119/2560] loss:10.227063884254264]\n",
      " Train step[120/2560] loss:10.223662622769673]\n",
      " Train step[121/2560] loss:10.220492883162065]\n",
      " Train step[122/2560] loss:10.217492728936868]\n",
      " Train step[123/2560] loss:10.214446649318788]\n",
      " Train step[124/2560] loss:10.21113206494239]\n",
      " Train step[125/2560] loss:10.208136672973632]\n",
      " Train step[126/2560] loss:10.204839766971649]\n",
      " Train step[127/2560] loss:10.2021136847068]\n",
      " Train step[128/2560] loss:10.199112519621849]\n",
      " Train step[129/2560] loss:10.196421638015629]\n",
      " Train step[130/2560] loss:10.193992468026968]\n",
      " Train step[131/2560] loss:10.191229347054286]\n",
      " Train step[132/2560] loss:10.188023321556322]\n",
      " Train step[133/2560] loss:10.18465933405367]\n",
      " Train step[134/2560] loss:10.181889498411719]\n",
      " Train step[135/2560] loss:10.178960658885815]\n",
      " Train step[136/2560] loss:10.176417596199933]\n",
      " Train step[137/2560] loss:10.17318618384591]\n",
      " Train step[138/2560] loss:10.170294042946637]\n",
      " Train step[139/2560] loss:10.167109386526423]\n",
      " Train step[140/2560] loss:10.163786792755127]\n",
      " Train step[141/2560] loss:10.161233489395034]\n",
      " Train step[142/2560] loss:10.158472000713079]\n",
      " Train step[143/2560] loss:10.155203499160447]\n",
      " Train step[144/2560] loss:10.152196910646227]\n",
      " Train step[145/2560] loss:10.14939087177145]\n",
      " Train step[146/2560] loss:10.146654011452036]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-972361fa1b80>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-45-44e439341ae0>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mtraining_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_preprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWord2VecDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConfig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword2idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbag_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-43-89a1be4e0782>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(training_dataset, config)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Epoch [{}/{}]\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mepoch_avg_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0msave_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_avg_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-42-14a54d286bc8>\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(model, training_loader, device, config)\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[0mavg_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\r\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Train step[{}/{}] loss:{}]\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mavg_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
